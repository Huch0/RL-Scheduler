{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "04f2142d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "import json\n",
    "import copy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.backends.backend_agg import FigureCanvasAgg\n",
    "\n",
    "from stable_baselines3 import A2C, PPO, DQN\n",
    "from stable_baselines3.common.env_checker import check_env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ec0d945a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def type_incoding(type):\n",
    "        type_code = {'A' : 0, 'B' : 1, 'C' : 2, 'D' : 3, 'E': 4, 'F' : 5, 'G' : 6, 'H' : 7, 'I' : 8, 'J' : 9, 'K' : 10, 'L' : 11, 'M' : 12, 'N' : 13, 'O' : 14, 'P' : 15, 'Q' : 16, 'R' : 17, 'S' : 18, 'T' : 19, 'U' : 20, 'V' : 21, 'W' : 22, 'X' : 23, 'Y' : 24, 'Z' : 25}\n",
    "        return type_code[type]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "96749977",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Resource():\n",
    "    def __init__(self, resouces_dictionary):\n",
    "        self.task_schedule = [] # (tasks)\n",
    "        self.name = resouces_dictionary['name'] \n",
    "        self.ability = self.ability_incoding(resouces_dictionary['ability']) # \"A, B, C, ...\"\n",
    "        self.reward = 0\n",
    "\n",
    "    def __str__(self):\n",
    "        # str_to_tasks = [str(task) for task in self.task_schedule]\n",
    "        # return f\"{self.name} : {str_to_tasks}\"\n",
    "        return f\"{self.name}\"\n",
    "    def ability_incoding(self, ability):\n",
    "        return [type_incoding(type) for type in ability]\n",
    "    \n",
    "    def can_process_task(self, task_type):\n",
    "        return task_type in self.ability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d19c85bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Order():\n",
    "    def __init__(self, order_dictionary):\n",
    "        self.name = order_dictionary['name']\n",
    "        self.color = order_dictionary['color']\n",
    "        self.task_queue = [Task(task_dictionary) for task_dictionary in order_dictionary['tasks']]\n",
    "        self.reward = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1daa3695",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Task():\n",
    "    def __init__(self, task_dictionary):\n",
    "        self.sequence = task_dictionary['sequence']\n",
    "        self.index = task_dictionary['index']\n",
    "        self.type = type_incoding(task_dictionary['type'])\n",
    "        self.predecessor = task_dictionary['predecessor']\n",
    "        self.earliest_start = task_dictionary['earliest_start']\n",
    "        self.duration = task_dictionary['duration']\n",
    "        self.start = task_dictionary['start']\n",
    "        self.finish = task_dictionary['finish']\n",
    "        self.resource = -1\n",
    "        self.color = \"\"\n",
    "        self.order = -1\n",
    "\n",
    "    def to_dict(self):\n",
    "        return {\n",
    "            'sequence': self.sequence,\n",
    "            'index' : self.index,\n",
    "            'type' : self.type,\n",
    "            'predecessor' : self.predecessor,\n",
    "            'earliest_start' : self.earliest_start,\n",
    "            'duration' : self.duration,\n",
    "            'start': self.start,\n",
    "            'finish': self.finish,\n",
    "            'resource': self.resource,\n",
    "            'color' : self.color,\n",
    "            'order' : self.order\n",
    "        }\n",
    "    def __str__(self):\n",
    "        return f\"order : {self.order}, step : {self.index} | ({self.start}, {self.finish})\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a2262643",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SchedulingEnv(gym.Env):\n",
    "    def load_resources(self, file_path):\n",
    "        resources = []\n",
    "\n",
    "        with open(file_path, 'r') as file:\n",
    "            data = json.load(file)\n",
    "\n",
    "        for resource_data in data[\"resources\"]:\n",
    "            resource = {}\n",
    "            resource['name'] = resource_data[\"name\"]\n",
    "            resource['ability'] = resource_data[\"type\"].split(', ')\n",
    "            resources.append(resource)\n",
    "\n",
    "        return resources\n",
    "    \n",
    "    def load_orders_new_version(self, file):\n",
    "        # Just in case we are reloading tasks\n",
    "        \n",
    "        orders = [] # 리턴할 용도\n",
    "        orders_new_version = [] # 파일 읽고 저장할 때 쓰는 용도\n",
    "        f = open(file)\n",
    "\n",
    "        # returns JSON object as  a dictionary\n",
    "        data = json.load(f)\n",
    "        f.close()\n",
    "        orders_new_version = data['orders']\n",
    "\n",
    "        for order in orders_new_version:\n",
    "            order_dictonary = {}\n",
    "            # Initial index of steps within order\n",
    "            order_dictonary['name'] = order['name']\n",
    "            order_dictonary['color'] = order['color']\n",
    "            earliestStart = order['earliest_start']\n",
    "\n",
    "            tasks = []\n",
    "            for task in order['tasks']:\n",
    "                predecessor = task['predecessor']\n",
    "                task_dictionary = {}\n",
    "                # Sequence is the scheduling order, the series of which defines a State or Node.\n",
    "                task_dictionary['sequence'] = None\n",
    "                task_dictionary['index'] = task['index']\n",
    "                task_dictionary['type'] = task['type']\n",
    "                if predecessor is None:\n",
    "                    task_dictionary['predecessor'] = None\n",
    "                    task_dictionary['earliest_start'] = earliestStart\n",
    "                else:\n",
    "                    task_dictionary['predecessor'] = predecessor\n",
    "                    task_dictionary['earliest_start'] = None\n",
    "                task_dictionary['duration'] = task['duration']\n",
    "                task_dictionary['start'] = None\n",
    "                task_dictionary['finish'] = None\n",
    "\n",
    "                tasks.append(task_dictionary)\n",
    "            \n",
    "            order_dictonary['tasks'] = tasks\n",
    "            orders.append(order_dictonary)\n",
    "\n",
    "        return orders\n",
    "\n",
    "    def __init__(self, resources = \"../resources/resources-10.json\", orders = \"../orders/converted_data.json\", render_mode=\"seaborn\"):\n",
    "        super(SchedulingEnv, self).__init__()\n",
    "\n",
    "        resources = self.load_resources(resources)\n",
    "        orders = self.load_orders_new_version(orders)\n",
    "        self.resources = [Resource(resource_info) for resource_info in resources]\n",
    "        self.orders = [Order(order_info) for order_info in orders]\n",
    "        len_resource = len(self.resources)\n",
    "        len_orders = len(self.orders)\n",
    "        # Reset 할 때 DeepCopy를 위해 원본을 저장해둠\n",
    "        self.original_orders = copy.deepcopy(self.orders)\n",
    "        self.original_resources = copy.deepcopy(self.resources)\n",
    "        self.original_tasks = copy.deepcopy([order.task_queue for order in self.orders])\n",
    "        self.num_tasks = sum([len(order.task_queue) for order in self.orders])\n",
    "\n",
    "        self.schedule_buffer = [-1 for _ in range(len(self.orders))]\n",
    "        self.state = None\n",
    "        self.legal_actions = None\n",
    "        self.action_space = spaces.MultiDiscrete([len_resource, len_orders])\n",
    "        self.observation_space = spaces.Dict({\n",
    "            \"action_mask\": spaces.MultiBinary([len_resource, len_orders]),\n",
    "            \"real_observation\": spaces.Box(low=-10, high=5000, shape=(len_orders, 4), dtype=np.float64)\n",
    "        })\n",
    "        \n",
    "        self.current_schedule = []\n",
    "        self.num_scheduled_tasks = 0\n",
    "        self.num_steps = 0\n",
    "        self.invalid_count = 0\n",
    "        self.last_finish_time = 0\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        \"\"\"\n",
    "        Important: the observation must be a numpy array\n",
    "        :return: (np.array)\n",
    "        \"\"\"\n",
    "        super().reset(seed=seed, options=options)\n",
    "\n",
    "        # 환경과 관련된 변수들     \n",
    "        self.orders = copy.deepcopy(self.original_orders)\n",
    "        self.resources = copy.deepcopy(self.original_resources)\\\n",
    "        \n",
    "        # 내부 동작을 위한 변수들\n",
    "        # self.state에 관한 추가설명 / Order 하나 당 가지는 정보는 아래와 같다\n",
    "        # 1. 남은 task 수\n",
    "        # 2. 다음으로 수행할 Task의 Type\n",
    "        # 3. 다음으로 수행할 Task의 earliest_start\n",
    "        # 4. 다음으로 수행할 Task의 duration\n",
    "        self.state = np.zeros((len(self.orders), 4), dtype=np.int32)\n",
    "        self.legal_actions = np.ones((len(self.resources), len(self.orders)), dtype=bool)       \n",
    "        self._update_schedule_buffer()\n",
    "        self._update_state()\n",
    "\n",
    "        # 기록을 위한 변수들\n",
    "        self.current_schedule = []\n",
    "        self.num_scheduled_tasks = 0\n",
    "        self.num_steps = 0\n",
    "        self.invalid_count = 0\n",
    "        self.last_finish_time = 0\n",
    "\n",
    "        info = {\n",
    "            'finish_time' : self.last_finish_time,\n",
    "            'invalid_count' : self.invalid_count,\n",
    "            'resources_reward' : [resource.reward for resource in self.resources],\n",
    "            'orders_reward' : [order.reward for order in self.orders],\n",
    "            'schedule_buffer' : self.schedule_buffer,\n",
    "            'current_schedule' : self.current_schedule\n",
    "               }\n",
    "\n",
    "        return self._get_observation(), info  # empty info dict\n",
    "\n",
    "    def step(self, action):\n",
    "        def is_error_action(act):\n",
    "            return act[0] < 0 or act[1] < 0 or act[0] >= len(self.resources) or act[1] >= len(self.orders)\n",
    "\n",
    "        if is_error_action(action):\n",
    "            raise ValueError(\n",
    "                f\"Received invalid action={action} which is not part of the action space\"\n",
    "            )\n",
    "\n",
    "        # error_action이 아니라면 step의 수를 증가시킨다\n",
    "        self.num_steps += 1\n",
    "        reward = -1\n",
    "        # 현재 아래 업데이트의 문제점 : Resource와 Task의 타입이 맞지 않아 False 처리를 한 이후 다시 True로 바뀔 수 있어야하는데 구현 하지 못했음\n",
    "        self._update_legal_actions()\n",
    "        if self.legal_actions[action[0]][action[1]]:\n",
    "            self._schedule_task(action)\n",
    "            self._update_schedule_buffer(action[1])\n",
    "            self._update_state()\n",
    "            self.last_finish_time = self._get_final_task_finish()\n",
    "            self._calculate_step_reward(action)\n",
    "            reward = self._calculate_total_reward()\n",
    "        else:\n",
    "            self.invalid_count += 1\n",
    "            \n",
    "        # 고로 다시 아래처럼 초기화함\n",
    "        self.legal_actions = np.ones((len(self.resources), len(self.orders)), dtype=bool)\n",
    "\n",
    "        # 모든 Order의 Task가 종료된 경우 Terminated를 True로 설정한다\n",
    "        # 또한 legal_actions가 전부 False인 경우도 Terminated를 True로 설정한다\n",
    "        terminated = all([order.task_queue[-1].finish is not None for order in self.orders]) or not np.any(self.legal_actions)\n",
    "        \n",
    "        if terminated:\n",
    "            reward += 10000/self._get_final_task_finish()\n",
    "\n",
    "        # 무한 루프를 방지하기 위한 조건\n",
    "        truncated = bool(self.num_steps == 10000)\n",
    "\n",
    "        # Optionally we can pass additional info, we are not using that for now\n",
    "        info = {\n",
    "            'finish_time' : self.last_finish_time,\n",
    "            'invalid_count' : self.invalid_count,\n",
    "            'resources_reward' : [resource.reward for resource in self.resources],\n",
    "            'orders_reward' : [order.reward for order in self.orders],\n",
    "            'schedule_buffer' : self.schedule_buffer,\n",
    "            'current_schedule' : self.current_schedule\n",
    "               }\n",
    "\n",
    "        return (\n",
    "            self._get_observation(),\n",
    "            reward,\n",
    "            terminated,\n",
    "            truncated,\n",
    "            info,\n",
    "        )\n",
    "    \n",
    "    def get_action_mask(self):\n",
    "        return self.legal_actions\n",
    "\n",
    "    def _update_legal_actions(self):\n",
    "        for order_index in range(len(self.orders)):\n",
    "        # 1. 선택된 Order의 모든 Task가 이미 종료된 경우\n",
    "            if self.schedule_buffer[order_index] < 0:\n",
    "                self.legal_actions[:, order_index] = False\n",
    "        \n",
    "        for resource_index in range(len(self.resources)):\n",
    "            # 2. 선택된 Resource가 선택된 Order의 Task의 Type을 처리할 수 없는 경우\n",
    "            resource = self.resources[resource_index]\n",
    "            for order_index in range(len(self.orders)):\n",
    "                order = self.orders[order_index]\n",
    "                task = order.task_queue[self.schedule_buffer[order_index]]\n",
    "                if not resource.can_process_task(task.type):\n",
    "                    self.legal_actions[resource_index, order_index] = False\n",
    "\n",
    "    def _update_state(self):\n",
    "        # state는 order의 수 * 4의 행렬이다\n",
    "        # 각 열에는 해당 Order의 Task에 대한 정보가 담겨있다\n",
    "        # 남은 task 수\n",
    "        # 다음으로 수행할 Task의 Duration\n",
    "        # 다음으로 수행할 Task의 Earlist_start\n",
    "        # 다음으로 수행할 Task의 Type\n",
    "        for i, order in enumerate(self.orders):\n",
    "            task_index = self.schedule_buffer[i]\n",
    "            if task_index < 0:\n",
    "                self.state[i] = np.zeros(4, dtype=np.int32)\n",
    "            else:\n",
    "                task = order.task_queue[task_index]\n",
    "                self.state[i] = [len(order.task_queue) - task_index, task.duration, task.earliest_start, task.type]\n",
    "\n",
    "    def _update_schedule_buffer(self, target_order = None):\n",
    "        # target_order은 매번 모든 Order를 보는 계산량을 줄이기 위해 설정할 변수\n",
    "        # None은 최초의 호출에서, 또는 Reset이 이뤄질 경우를 위해 존재\n",
    "        if target_order == None:\n",
    "            buffer_index = 0\n",
    "            \n",
    "            for order in self.orders:\n",
    "                # Assume order['steps'] is a list of tasks for the current order\n",
    "                \n",
    "                selected_task_index = -1\n",
    "                \n",
    "                for i in range(len(order.task_queue)):\n",
    "                    # 아직 스케줄링을 시작하지 않은 Task를 찾는다\n",
    "                    if order.task_queue[i].finish is None:\n",
    "                        selected_task_index = i\n",
    "                        break\n",
    "                # 스케줄링 하지 않은 Task를 발견했다면        \n",
    "                if selected_task_index >= 0:\n",
    "                    selected_task = order.task_queue[selected_task_index]\n",
    "        \n",
    "                    # 만약 초기 시작 제한이 없다면 \n",
    "                    # 초기 시작 제한을 이전 Task의 Finish Time으로 걸어주고 버퍼에 등록한다.\n",
    "                    if selected_task.earliest_start is None:\n",
    "                        if selected_task_index > 0:\n",
    "                            selected_task.earliest_start = order.task_queue[selected_task_index-1].finish\n",
    "                \n",
    "                self.schedule_buffer[buffer_index] = selected_task_index\n",
    "                buffer_index += 1\n",
    "                \n",
    "        # Action으로 인해 봐야할 버퍼의 인덱스가 정해짐\n",
    "        else:\n",
    "            selected_task_index = -1\n",
    "            order = self.orders[target_order]\n",
    "            for i in range(len(order.task_queue)):\n",
    "                # 아직 스케줄링을 시작하지 않은 Task를 찾는다\n",
    "                if order.task_queue[i].finish is None:\n",
    "                    selected_task_index = i\n",
    "                    break\n",
    "            if selected_task_index >= 0:\n",
    "                    selected_task = order.task_queue[selected_task_index]\n",
    "                    if selected_task.earliest_start is None:\n",
    "                        if selected_task_index > 0:\n",
    "                            selected_task.earliest_start = order.task_queue[selected_task_index-1].finish\n",
    "            \n",
    "            self.schedule_buffer[target_order] = selected_task_index\n",
    "        \n",
    "    def _schedule_task(self, action):\n",
    "        # Implement the scheduling logic based on the action\n",
    "        # You need to update the start and finish times of the tasks\n",
    "        # based on the selected task index (action) and the current state.\n",
    "\n",
    "        # Example: updating start and finish times\n",
    "        selected_resource = self.resources[action[0]]\n",
    "        selected_order = self.orders[action[1]]\n",
    "        selected_task = selected_order.task_queue[self.schedule_buffer[action[1]]]\n",
    "        task_earliest_start = selected_task.earliest_start\n",
    "        task_index = selected_task.index\n",
    "        task_duration = selected_task.duration\n",
    "        resource_tasks = sorted(selected_resource.task_schedule, key=lambda task: task.start)\n",
    "\n",
    "        open_windows = []\n",
    "        start_window = 0\n",
    "        last_alloc = 0\n",
    "\n",
    "        for scheduled_task in resource_tasks:\n",
    "            resource_init = scheduled_task.start\n",
    "\n",
    "            if resource_init > start_window:\n",
    "                open_windows.append([start_window, resource_init])\n",
    "            start_window = scheduled_task.finish\n",
    "\n",
    "            last_alloc = max(last_alloc, start_window)\n",
    "\n",
    "        # Fit the task within the first possible window\n",
    "        window_found = False\n",
    "        if task_earliest_start is None:\n",
    "            task_earliest_start = 0\n",
    "\n",
    "        for window in open_windows:\n",
    "            # Task could start before the open window closes\n",
    "            if task_earliest_start <= window[1]:\n",
    "                # Now let's see if it fits there\n",
    "                potential_start = max(task_earliest_start, window[0])\n",
    "                if potential_start + task_duration <= window[1]:\n",
    "                    # Task fits into the window\n",
    "                    min_earliest_start = potential_start\n",
    "                    window_found = True\n",
    "                    break\n",
    "\n",
    "        # If no window was found, schedule it after the end of the last task on the resource\n",
    "        if not window_found:\n",
    "            if task_earliest_start > 0:\n",
    "                min_earliest_start = max(task_earliest_start, last_alloc)\n",
    "            else:\n",
    "                min_earliest_start = last_alloc\n",
    "\n",
    "        # schedule it\n",
    "        selected_task.sequence = self.num_scheduled_tasks + 1\n",
    "        selected_task.start = min_earliest_start\n",
    "        selected_task.finish = min_earliest_start + task_duration\n",
    "        selected_task.resource = action[0]\n",
    "\n",
    "        # 사실 여기서 color랑 order를 주는건 적절치 않은 코드임!!!!\n",
    "        selected_task.color = self.orders[action[1]].color\n",
    "        selected_task.order = action[1]\n",
    "\n",
    "        self.current_schedule.append(selected_task)\n",
    "        selected_resource.task_schedule.append(selected_task)\n",
    "        self.num_scheduled_tasks += 1\n",
    "        return\n",
    "\n",
    "    def _get_final_task_finish(self):\n",
    "        return max(self.current_schedule, key=lambda x: x.finish).finish\n",
    "        \n",
    "    def _calculate_total_reward(self):\n",
    "        scale_factor = 0\n",
    "        for task in self.current_schedule:\n",
    "            scale_factor += task.duration\n",
    "        # reward = reward / self._get_final_task_finish()\n",
    "        return sum([order.reward for order in self.orders])/scale_factor #+ sum([resource.reward for resource in self.resources])) / scale_factor\n",
    "    \n",
    "    def _calculate_step_reward(self, action):\n",
    "        # Hall 리워드 초기화\n",
    "        hall_resource = 0\n",
    "        hall_order = 0\n",
    "        \n",
    "        # 선택된 리소스와 주문\n",
    "        selected_resource = self.resources[action[0]]\n",
    "        selected_order = self.orders[action[1]]\n",
    "    \n",
    "        # 선택된 리소스의 스케줄링된 Task들\n",
    "        scheduled_tasks = sorted(selected_resource.task_schedule, key=lambda task: task.start)\n",
    "        sum_duration = 0\n",
    "        for task in scheduled_tasks:\n",
    "            sum_duration += task.duration\n",
    "        \n",
    "        if len(scheduled_tasks) >= 2:\n",
    "            # 리소스의 스케줄링된 Task 사이의 간격을 계산하여 Hall 리워드에 더합니다.\n",
    "            for i in range(1, len(scheduled_tasks)):\n",
    "                gap = scheduled_tasks[i].start - scheduled_tasks[i - 1].finish\n",
    "                hall_resource += gap\n",
    "\n",
    "        # 선택된 주문의 수행된 Task들\n",
    "        performed_tasks = [task for task in selected_order.task_queue if task.finish is not None]\n",
    "        sum_performed_duration = 0\n",
    "        for task in performed_tasks:\n",
    "            sum_performed_duration += task.duration\n",
    "        if len(performed_tasks) >= 2:\n",
    "            # 주문의 수행된 Task 사이의 간격을 계산하여 Hall 리워드에 더합니다.\n",
    "            for i in range(1, len(performed_tasks)):\n",
    "                gap = performed_tasks[i].start - performed_tasks[i - 1].finish\n",
    "                hall_order += gap\n",
    "        \n",
    "        selected_resource.reward += (sum_duration - hall_resource)\n",
    "        selected_order.reward += (sum_performed_duration - hall_order)\n",
    "\n",
    "    def _get_observation(self):\n",
    "        observation = {\n",
    "            'action_mask': self.legal_actions,\n",
    "            'real_observation': self.state\n",
    "            }\n",
    "\n",
    "        return observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f20bc4ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = SchedulingEnv()\n",
    "check_env(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96011d99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 95.2     |\n",
      "|    ep_rew_mean        | 11.6     |\n",
      "| time/                 |          |\n",
      "|    fps                | 291      |\n",
      "|    iterations         | 100      |\n",
      "|    time_elapsed       | 1        |\n",
      "|    total_timesteps    | 500      |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -4.9     |\n",
      "|    explained_variance | 0.678    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 99       |\n",
      "|    policy_loss        | 4.67     |\n",
      "|    value_loss         | 1.13     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 97.2     |\n",
      "|    ep_rew_mean        | 0.727    |\n",
      "| time/                 |          |\n",
      "|    fps                | 295      |\n",
      "|    iterations         | 200      |\n",
      "|    time_elapsed       | 3        |\n",
      "|    total_timesteps    | 1000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -4.83    |\n",
      "|    explained_variance | -0.19    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 199      |\n",
      "|    policy_loss        | 18.6     |\n",
      "|    value_loss         | 15.3     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 106      |\n",
      "|    ep_rew_mean        | -6.77    |\n",
      "| time/                 |          |\n",
      "|    fps                | 286      |\n",
      "|    iterations         | 300      |\n",
      "|    time_elapsed       | 5        |\n",
      "|    total_timesteps    | 1500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -4.84    |\n",
      "|    explained_variance | 0.83     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 299      |\n",
      "|    policy_loss        | 10       |\n",
      "|    value_loss         | 4.61     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 103      |\n",
      "|    ep_rew_mean        | -1.83    |\n",
      "| time/                 |          |\n",
      "|    fps                | 285      |\n",
      "|    iterations         | 400      |\n",
      "|    time_elapsed       | 6        |\n",
      "|    total_timesteps    | 2000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -4.88    |\n",
      "|    explained_variance | -5.27    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 399      |\n",
      "|    policy_loss        | -0.862   |\n",
      "|    value_loss         | 1.8      |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 97.4     |\n",
      "|    ep_rew_mean        | 3.88     |\n",
      "| time/                 |          |\n",
      "|    fps                | 282      |\n",
      "|    iterations         | 500      |\n",
      "|    time_elapsed       | 8        |\n",
      "|    total_timesteps    | 2500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -4.89    |\n",
      "|    explained_variance | 0.101    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 499      |\n",
      "|    policy_loss        | -1.66    |\n",
      "|    value_loss         | 0.875    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 98.7     |\n",
      "|    ep_rew_mean        | 1.76     |\n",
      "| time/                 |          |\n",
      "|    fps                | 284      |\n",
      "|    iterations         | 600      |\n",
      "|    time_elapsed       | 10       |\n",
      "|    total_timesteps    | 3000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -4.76    |\n",
      "|    explained_variance | -4.07    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 599      |\n",
      "|    policy_loss        | -3.39    |\n",
      "|    value_loss         | 3.6      |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 95.9     |\n",
      "|    ep_rew_mean        | 2.99     |\n",
      "| time/                 |          |\n",
      "|    fps                | 282      |\n",
      "|    iterations         | 700      |\n",
      "|    time_elapsed       | 12       |\n",
      "|    total_timesteps    | 3500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -4.62    |\n",
      "|    explained_variance | -0.504   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 699      |\n",
      "|    policy_loss        | 8.33     |\n",
      "|    value_loss         | 6.14     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 95.8     |\n",
      "|    ep_rew_mean        | 3.28     |\n",
      "| time/                 |          |\n",
      "|    fps                | 280      |\n",
      "|    iterations         | 800      |\n",
      "|    time_elapsed       | 14       |\n",
      "|    total_timesteps    | 4000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -4.91    |\n",
      "|    explained_variance | -2.62    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 799      |\n",
      "|    policy_loss        | -14.6    |\n",
      "|    value_loss         | 9.45     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 96.3     |\n",
      "|    ep_rew_mean        | 2.38     |\n",
      "| time/                 |          |\n",
      "|    fps                | 281      |\n",
      "|    iterations         | 900      |\n",
      "|    time_elapsed       | 16       |\n",
      "|    total_timesteps    | 4500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -4.81    |\n",
      "|    explained_variance | -1.35    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 899      |\n",
      "|    policy_loss        | 12       |\n",
      "|    value_loss         | 9.21     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 94.2     |\n",
      "|    ep_rew_mean        | 5.35     |\n",
      "| time/                 |          |\n",
      "|    fps                | 280      |\n",
      "|    iterations         | 1000     |\n",
      "|    time_elapsed       | 17       |\n",
      "|    total_timesteps    | 5000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -4.79    |\n",
      "|    explained_variance | -1.66    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 999      |\n",
      "|    policy_loss        | -3.34    |\n",
      "|    value_loss         | 3.08     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 92.9     |\n",
      "|    ep_rew_mean        | 7.48     |\n",
      "| time/                 |          |\n",
      "|    fps                | 280      |\n",
      "|    iterations         | 1100     |\n",
      "|    time_elapsed       | 19       |\n",
      "|    total_timesteps    | 5500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -4.63    |\n",
      "|    explained_variance | 0.68     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1099     |\n",
      "|    policy_loss        | 3.59     |\n",
      "|    value_loss         | 1.48     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 93.4     |\n",
      "|    ep_rew_mean        | 6.65     |\n",
      "| time/                 |          |\n",
      "|    fps                | 280      |\n",
      "|    iterations         | 1200     |\n",
      "|    time_elapsed       | 21       |\n",
      "|    total_timesteps    | 6000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -4.79    |\n",
      "|    explained_variance | 0.568    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1199     |\n",
      "|    policy_loss        | 32.6     |\n",
      "|    value_loss         | 55.8     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 93.8     |\n",
      "|    ep_rew_mean        | 6.69     |\n",
      "| time/                 |          |\n",
      "|    fps                | 280      |\n",
      "|    iterations         | 1300     |\n",
      "|    time_elapsed       | 23       |\n",
      "|    total_timesteps    | 6500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -4.78    |\n",
      "|    explained_variance | 0.0824   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1299     |\n",
      "|    policy_loss        | 13.4     |\n",
      "|    value_loss         | 9.07     |\n",
      "------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 92.7     |\n",
      "|    ep_rew_mean        | 8.03     |\n",
      "| time/                 |          |\n",
      "|    fps                | 281      |\n",
      "|    iterations         | 1400     |\n",
      "|    time_elapsed       | 24       |\n",
      "|    total_timesteps    | 7000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -4.86    |\n",
      "|    explained_variance | -4.51    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1399     |\n",
      "|    policy_loss        | -11.5    |\n",
      "|    value_loss         | 9.72     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 91.8     |\n",
      "|    ep_rew_mean        | 9.72     |\n",
      "| time/                 |          |\n",
      "|    fps                | 279      |\n",
      "|    iterations         | 1500     |\n",
      "|    time_elapsed       | 26       |\n",
      "|    total_timesteps    | 7500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -4.8     |\n",
      "|    explained_variance | -6.01    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1499     |\n",
      "|    policy_loss        | -20.6    |\n",
      "|    value_loss         | 23.3     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 91       |\n",
      "|    ep_rew_mean        | 10.6     |\n",
      "| time/                 |          |\n",
      "|    fps                | 279      |\n",
      "|    iterations         | 1600     |\n",
      "|    time_elapsed       | 28       |\n",
      "|    total_timesteps    | 8000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -4.8     |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1599     |\n",
      "|    policy_loss        | -14.6    |\n",
      "|    value_loss         | 10       |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 90.4     |\n",
      "|    ep_rew_mean        | 11.4     |\n",
      "| time/                 |          |\n",
      "|    fps                | 279      |\n",
      "|    iterations         | 1700     |\n",
      "|    time_elapsed       | 30       |\n",
      "|    total_timesteps    | 8500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -4.67    |\n",
      "|    explained_variance | -0.239   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1699     |\n",
      "|    policy_loss        | 44.9     |\n",
      "|    value_loss         | 147      |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 90.2     |\n",
      "|    ep_rew_mean        | 12       |\n",
      "| time/                 |          |\n",
      "|    fps                | 279      |\n",
      "|    iterations         | 1800     |\n",
      "|    time_elapsed       | 32       |\n",
      "|    total_timesteps    | 9000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -4.61    |\n",
      "|    explained_variance | -31.3    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1799     |\n",
      "|    policy_loss        | -26.5    |\n",
      "|    value_loss         | 61.6     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 88.5     |\n",
      "|    ep_rew_mean        | 13.7     |\n",
      "| time/                 |          |\n",
      "|    fps                | 278      |\n",
      "|    iterations         | 1900     |\n",
      "|    time_elapsed       | 34       |\n",
      "|    total_timesteps    | 9500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -4.78    |\n",
      "|    explained_variance | -23.9    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1899     |\n",
      "|    policy_loss        | -13.2    |\n",
      "|    value_loss         | 20.4     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 85.8     |\n",
      "|    ep_rew_mean        | 17.2     |\n",
      "| time/                 |          |\n",
      "|    fps                | 276      |\n",
      "|    iterations         | 2000     |\n",
      "|    time_elapsed       | 36       |\n",
      "|    total_timesteps    | 10000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -4.5     |\n",
      "|    explained_variance | -12.8    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1999     |\n",
      "|    policy_loss        | 12.8     |\n",
      "|    value_loss         | 28.1     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 85       |\n",
      "|    ep_rew_mean        | 17.4     |\n",
      "| time/                 |          |\n",
      "|    fps                | 276      |\n",
      "|    iterations         | 2100     |\n",
      "|    time_elapsed       | 38       |\n",
      "|    total_timesteps    | 10500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -4.58    |\n",
      "|    explained_variance | -1.82    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 2099     |\n",
      "|    policy_loss        | -4.19    |\n",
      "|    value_loss         | 2.24     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 84.3     |\n",
      "|    ep_rew_mean        | 18       |\n",
      "| time/                 |          |\n",
      "|    fps                | 275      |\n",
      "|    iterations         | 2200     |\n",
      "|    time_elapsed       | 39       |\n",
      "|    total_timesteps    | 11000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -4.55    |\n",
      "|    explained_variance | -1.09    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 2199     |\n",
      "|    policy_loss        | 74.8     |\n",
      "|    value_loss         | 391      |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 82.8     |\n",
      "|    ep_rew_mean        | 20.4     |\n",
      "| time/                 |          |\n",
      "|    fps                | 276      |\n",
      "|    iterations         | 2300     |\n",
      "|    time_elapsed       | 41       |\n",
      "|    total_timesteps    | 11500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -4.74    |\n",
      "|    explained_variance | 5.96e-08 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 2299     |\n",
      "|    policy_loss        | -14.7    |\n",
      "|    value_loss         | 10.6     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 82       |\n",
      "|    ep_rew_mean        | 22       |\n",
      "| time/                 |          |\n",
      "|    fps                | 275      |\n",
      "|    iterations         | 2400     |\n",
      "|    time_elapsed       | 43       |\n",
      "|    total_timesteps    | 12000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -4.43    |\n",
      "|    explained_variance | 0.151    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 2399     |\n",
      "|    policy_loss        | -12.1    |\n",
      "|    value_loss         | 4.88     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 81.2     |\n",
      "|    ep_rew_mean        | 23.4     |\n",
      "| time/                 |          |\n",
      "|    fps                | 272      |\n",
      "|    iterations         | 2500     |\n",
      "|    time_elapsed       | 45       |\n",
      "|    total_timesteps    | 12500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -4.61    |\n",
      "|    explained_variance | -20.6    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 2499     |\n",
      "|    policy_loss        | -51.3    |\n",
      "|    value_loss         | 119      |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 80.3     |\n",
      "|    ep_rew_mean        | 24.3     |\n",
      "| time/                 |          |\n",
      "|    fps                | 271      |\n",
      "|    iterations         | 2600     |\n",
      "|    time_elapsed       | 47       |\n",
      "|    total_timesteps    | 13000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -4.25    |\n",
      "|    explained_variance | -141     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 2599     |\n",
      "|    policy_loss        | -59.1    |\n",
      "|    value_loss         | 232      |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 80.1     |\n",
      "|    ep_rew_mean        | 25.2     |\n",
      "| time/                 |          |\n",
      "|    fps                | 268      |\n",
      "|    iterations         | 2700     |\n",
      "|    time_elapsed       | 50       |\n",
      "|    total_timesteps    | 13500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -4.24    |\n",
      "|    explained_variance | 0.498    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 2699     |\n",
      "|    policy_loss        | 23.1     |\n",
      "|    value_loss         | 104      |\n",
      "------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 79.4     |\n",
      "|    ep_rew_mean        | 26.6     |\n",
      "| time/                 |          |\n",
      "|    fps                | 268      |\n",
      "|    iterations         | 2800     |\n",
      "|    time_elapsed       | 52       |\n",
      "|    total_timesteps    | 14000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -4.59    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 2799     |\n",
      "|    policy_loss        | -11      |\n",
      "|    value_loss         | 7.47     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 78       |\n",
      "|    ep_rew_mean        | 28.3     |\n",
      "| time/                 |          |\n",
      "|    fps                | 267      |\n",
      "|    iterations         | 2900     |\n",
      "|    time_elapsed       | 54       |\n",
      "|    total_timesteps    | 14500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -4.73    |\n",
      "|    explained_variance | -2.21    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 2899     |\n",
      "|    policy_loss        | -40.1    |\n",
      "|    value_loss         | 87       |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 77.9     |\n",
      "|    ep_rew_mean        | 28.8     |\n",
      "| time/                 |          |\n",
      "|    fps                | 267      |\n",
      "|    iterations         | 3000     |\n",
      "|    time_elapsed       | 55       |\n",
      "|    total_timesteps    | 15000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -4.25    |\n",
      "|    explained_variance | 0.483    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 2999     |\n",
      "|    policy_loss        | 5.92     |\n",
      "|    value_loss         | 3.12     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 77.3     |\n",
      "|    ep_rew_mean        | 29.5     |\n",
      "| time/                 |          |\n",
      "|    fps                | 268      |\n",
      "|    iterations         | 3100     |\n",
      "|    time_elapsed       | 57       |\n",
      "|    total_timesteps    | 15500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -4.41    |\n",
      "|    explained_variance | -0.00193 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 3099     |\n",
      "|    policy_loss        | 10.8     |\n",
      "|    value_loss         | 7.83     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 77.7     |\n",
      "|    ep_rew_mean        | 29.6     |\n",
      "| time/                 |          |\n",
      "|    fps                | 268      |\n",
      "|    iterations         | 3200     |\n",
      "|    time_elapsed       | 59       |\n",
      "|    total_timesteps    | 16000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -4.53    |\n",
      "|    explained_variance | -36.9    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 3199     |\n",
      "|    policy_loss        | -36.8    |\n",
      "|    value_loss         | 102      |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 77.2     |\n",
      "|    ep_rew_mean        | 30.7     |\n",
      "| time/                 |          |\n",
      "|    fps                | 268      |\n",
      "|    iterations         | 3300     |\n",
      "|    time_elapsed       | 61       |\n",
      "|    total_timesteps    | 16500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -4.63    |\n",
      "|    explained_variance | -0.357   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 3299     |\n",
      "|    policy_loss        | -57.6    |\n",
      "|    value_loss         | 145      |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 77        |\n",
      "|    ep_rew_mean        | 31.8      |\n",
      "| time/                 |           |\n",
      "|    fps                | 269       |\n",
      "|    iterations         | 3400      |\n",
      "|    time_elapsed       | 63        |\n",
      "|    total_timesteps    | 17000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -4.6      |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 3399      |\n",
      "|    policy_loss        | -15.2     |\n",
      "|    value_loss         | 9.86      |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 78.4     |\n",
      "|    ep_rew_mean        | 30.9     |\n",
      "| time/                 |          |\n",
      "|    fps                | 269      |\n",
      "|    iterations         | 3500     |\n",
      "|    time_elapsed       | 64       |\n",
      "|    total_timesteps    | 17500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -4.35    |\n",
      "|    explained_variance | 0.33     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 3499     |\n",
      "|    policy_loss        | 7.77     |\n",
      "|    value_loss         | 4.88     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 78.8     |\n",
      "|    ep_rew_mean        | 31.9     |\n",
      "| time/                 |          |\n",
      "|    fps                | 270      |\n",
      "|    iterations         | 3600     |\n",
      "|    time_elapsed       | 66       |\n",
      "|    total_timesteps    | 18000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -4.36    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 3599     |\n",
      "|    policy_loss        | 105      |\n",
      "|    value_loss         | 606      |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 78       |\n",
      "|    ep_rew_mean        | 34.2     |\n",
      "| time/                 |          |\n",
      "|    fps                | 270      |\n",
      "|    iterations         | 3700     |\n",
      "|    time_elapsed       | 68       |\n",
      "|    total_timesteps    | 18500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -4.67    |\n",
      "|    explained_variance | 0.2      |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 3699     |\n",
      "|    policy_loss        | -5.19    |\n",
      "|    value_loss         | 2.99     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 78       |\n",
      "|    ep_rew_mean        | 34.9     |\n",
      "| time/                 |          |\n",
      "|    fps                | 271      |\n",
      "|    iterations         | 3800     |\n",
      "|    time_elapsed       | 70       |\n",
      "|    total_timesteps    | 19000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -4.6     |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 3799     |\n",
      "|    policy_loss        | -10.5    |\n",
      "|    value_loss         | 6.87     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 78.4     |\n",
      "|    ep_rew_mean        | 35.3     |\n",
      "| time/                 |          |\n",
      "|    fps                | 271      |\n",
      "|    iterations         | 3900     |\n",
      "|    time_elapsed       | 71       |\n",
      "|    total_timesteps    | 19500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -3.63    |\n",
      "|    explained_variance | -0.137   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 3899     |\n",
      "|    policy_loss        | 14.6     |\n",
      "|    value_loss         | 15.7     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 77.9     |\n",
      "|    ep_rew_mean        | 35.8     |\n",
      "| time/                 |          |\n",
      "|    fps                | 272      |\n",
      "|    iterations         | 4000     |\n",
      "|    time_elapsed       | 73       |\n",
      "|    total_timesteps    | 20000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -3.71    |\n",
      "|    explained_variance | 0.526    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 3999     |\n",
      "|    policy_loss        | -8.3     |\n",
      "|    value_loss         | 8.49     |\n",
      "------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 77.5     |\n",
      "|    ep_rew_mean        | 37.7     |\n",
      "| time/                 |          |\n",
      "|    fps                | 272      |\n",
      "|    iterations         | 4100     |\n",
      "|    time_elapsed       | 75       |\n",
      "|    total_timesteps    | 20500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -3.95    |\n",
      "|    explained_variance | 0.5      |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 4099     |\n",
      "|    policy_loss        | -65.7    |\n",
      "|    value_loss         | 286      |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 77.3      |\n",
      "|    ep_rew_mean        | 38.4      |\n",
      "| time/                 |           |\n",
      "|    fps                | 272       |\n",
      "|    iterations         | 4200      |\n",
      "|    time_elapsed       | 77        |\n",
      "|    total_timesteps    | 21000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -4.58     |\n",
      "|    explained_variance | -0.000601 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 4199      |\n",
      "|    policy_loss        | -0.625    |\n",
      "|    value_loss         | 0.863     |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 76.4     |\n",
      "|    ep_rew_mean        | 39.2     |\n",
      "| time/                 |          |\n",
      "|    fps                | 272      |\n",
      "|    iterations         | 4300     |\n",
      "|    time_elapsed       | 78       |\n",
      "|    total_timesteps    | 21500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -4.53    |\n",
      "|    explained_variance | -44.8    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 4299     |\n",
      "|    policy_loss        | -2.51    |\n",
      "|    value_loss         | 43.4     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 75.8     |\n",
      "|    ep_rew_mean        | 39.8     |\n",
      "| time/                 |          |\n",
      "|    fps                | 271      |\n",
      "|    iterations         | 4400     |\n",
      "|    time_elapsed       | 80       |\n",
      "|    total_timesteps    | 22000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -4.41    |\n",
      "|    explained_variance | -26      |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 4399     |\n",
      "|    policy_loss        | -14      |\n",
      "|    value_loss         | 16.8     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 75       |\n",
      "|    ep_rew_mean        | 40.3     |\n",
      "| time/                 |          |\n",
      "|    fps                | 272      |\n",
      "|    iterations         | 4500     |\n",
      "|    time_elapsed       | 82       |\n",
      "|    total_timesteps    | 22500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -3.72    |\n",
      "|    explained_variance | -1.67    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 4499     |\n",
      "|    policy_loss        | -10.9    |\n",
      "|    value_loss         | 18.4     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 74.9     |\n",
      "|    ep_rew_mean        | 41.3     |\n",
      "| time/                 |          |\n",
      "|    fps                | 272      |\n",
      "|    iterations         | 4600     |\n",
      "|    time_elapsed       | 84       |\n",
      "|    total_timesteps    | 23000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -4.21    |\n",
      "|    explained_variance | -7.08    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 4599     |\n",
      "|    policy_loss        | -28.7    |\n",
      "|    value_loss         | 190      |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 74       |\n",
      "|    ep_rew_mean        | 42.7     |\n",
      "| time/                 |          |\n",
      "|    fps                | 273      |\n",
      "|    iterations         | 4700     |\n",
      "|    time_elapsed       | 85       |\n",
      "|    total_timesteps    | 23500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -3.57    |\n",
      "|    explained_variance | 0.452    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 4699     |\n",
      "|    policy_loss        | 12.4     |\n",
      "|    value_loss         | 12.9     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 73.4     |\n",
      "|    ep_rew_mean        | 42.8     |\n",
      "| time/                 |          |\n",
      "|    fps                | 274      |\n",
      "|    iterations         | 4800     |\n",
      "|    time_elapsed       | 87       |\n",
      "|    total_timesteps    | 24000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -4.37    |\n",
      "|    explained_variance | -6.63    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 4799     |\n",
      "|    policy_loss        | 42.3     |\n",
      "|    value_loss         | 133      |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 72.4     |\n",
      "|    ep_rew_mean        | 43.1     |\n",
      "| time/                 |          |\n",
      "|    fps                | 275      |\n",
      "|    iterations         | 4900     |\n",
      "|    time_elapsed       | 88       |\n",
      "|    total_timesteps    | 24500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -4.22    |\n",
      "|    explained_variance | -0.615   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 4899     |\n",
      "|    policy_loss        | 7.91     |\n",
      "|    value_loss         | 3.53     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 71.7     |\n",
      "|    ep_rew_mean        | 43.8     |\n",
      "| time/                 |          |\n",
      "|    fps                | 276      |\n",
      "|    iterations         | 5000     |\n",
      "|    time_elapsed       | 90       |\n",
      "|    total_timesteps    | 25000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -4.07    |\n",
      "|    explained_variance | -0.353   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 4999     |\n",
      "|    policy_loss        | 28.7     |\n",
      "|    value_loss         | 271      |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 71.6     |\n",
      "|    ep_rew_mean        | 43.4     |\n",
      "| time/                 |          |\n",
      "|    fps                | 276      |\n",
      "|    iterations         | 5100     |\n",
      "|    time_elapsed       | 92       |\n",
      "|    total_timesteps    | 25500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -3.97    |\n",
      "|    explained_variance | 0.839    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 5099     |\n",
      "|    policy_loss        | 3.79     |\n",
      "|    value_loss         | 1.43     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 70.8     |\n",
      "|    ep_rew_mean        | 43.9     |\n",
      "| time/                 |          |\n",
      "|    fps                | 277      |\n",
      "|    iterations         | 5200     |\n",
      "|    time_elapsed       | 93       |\n",
      "|    total_timesteps    | 26000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -4.03    |\n",
      "|    explained_variance | -2.65    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 5199     |\n",
      "|    policy_loss        | -8.52    |\n",
      "|    value_loss         | 9.15     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 70       |\n",
      "|    ep_rew_mean        | 44.1     |\n",
      "| time/                 |          |\n",
      "|    fps                | 277      |\n",
      "|    iterations         | 5300     |\n",
      "|    time_elapsed       | 95       |\n",
      "|    total_timesteps    | 26500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -3.72    |\n",
      "|    explained_variance | -0.132   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 5299     |\n",
      "|    policy_loss        | 19.1     |\n",
      "|    value_loss         | 30.8     |\n",
      "------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 69.7     |\n",
      "|    ep_rew_mean        | 44.7     |\n",
      "| time/                 |          |\n",
      "|    fps                | 278      |\n",
      "|    iterations         | 5400     |\n",
      "|    time_elapsed       | 97       |\n",
      "|    total_timesteps    | 27000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -4.12    |\n",
      "|    explained_variance | -24.4    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 5399     |\n",
      "|    policy_loss        | 1.85     |\n",
      "|    value_loss         | 24.8     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 69.8     |\n",
      "|    ep_rew_mean        | 44.5     |\n",
      "| time/                 |          |\n",
      "|    fps                | 278      |\n",
      "|    iterations         | 5500     |\n",
      "|    time_elapsed       | 98       |\n",
      "|    total_timesteps    | 27500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -4.5     |\n",
      "|    explained_variance | -41.9    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 5499     |\n",
      "|    policy_loss        | 43.2     |\n",
      "|    value_loss         | 266      |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 69.9     |\n",
      "|    ep_rew_mean        | 44.6     |\n",
      "| time/                 |          |\n",
      "|    fps                | 278      |\n",
      "|    iterations         | 5600     |\n",
      "|    time_elapsed       | 100      |\n",
      "|    total_timesteps    | 28000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -4.4     |\n",
      "|    explained_variance | -0.614   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 5599     |\n",
      "|    policy_loss        | 12.3     |\n",
      "|    value_loss         | 12.2     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 69.6     |\n",
      "|    ep_rew_mean        | 45       |\n",
      "| time/                 |          |\n",
      "|    fps                | 278      |\n",
      "|    iterations         | 5700     |\n",
      "|    time_elapsed       | 102      |\n",
      "|    total_timesteps    | 28500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -4.29    |\n",
      "|    explained_variance | -7.2     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 5699     |\n",
      "|    policy_loss        | 12       |\n",
      "|    value_loss         | 11.1     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 69.2     |\n",
      "|    ep_rew_mean        | 45.9     |\n",
      "| time/                 |          |\n",
      "|    fps                | 278      |\n",
      "|    iterations         | 5800     |\n",
      "|    time_elapsed       | 104      |\n",
      "|    total_timesteps    | 29000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -4.08    |\n",
      "|    explained_variance | -18.9    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 5799     |\n",
      "|    policy_loss        | -15      |\n",
      "|    value_loss         | 37.5     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 69       |\n",
      "|    ep_rew_mean        | 45.4     |\n",
      "| time/                 |          |\n",
      "|    fps                | 279      |\n",
      "|    iterations         | 5900     |\n",
      "|    time_elapsed       | 105      |\n",
      "|    total_timesteps    | 29500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -4.21    |\n",
      "|    explained_variance | -13.8    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 5899     |\n",
      "|    policy_loss        | -34.2    |\n",
      "|    value_loss         | 101      |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 68.5     |\n",
      "|    ep_rew_mean        | 45       |\n",
      "| time/                 |          |\n",
      "|    fps                | 279      |\n",
      "|    iterations         | 6000     |\n",
      "|    time_elapsed       | 107      |\n",
      "|    total_timesteps    | 30000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -3.76    |\n",
      "|    explained_variance | -1.45    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 5999     |\n",
      "|    policy_loss        | -38.8    |\n",
      "|    value_loss         | 138      |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 68.1     |\n",
      "|    ep_rew_mean        | 45.3     |\n",
      "| time/                 |          |\n",
      "|    fps                | 280      |\n",
      "|    iterations         | 6100     |\n",
      "|    time_elapsed       | 108      |\n",
      "|    total_timesteps    | 30500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -3.97    |\n",
      "|    explained_variance | -3.79    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 6099     |\n",
      "|    policy_loss        | 11.6     |\n",
      "|    value_loss         | 8.29     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 67.9     |\n",
      "|    ep_rew_mean        | 45.3     |\n",
      "| time/                 |          |\n",
      "|    fps                | 280      |\n",
      "|    iterations         | 6200     |\n",
      "|    time_elapsed       | 110      |\n",
      "|    total_timesteps    | 31000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -4.42    |\n",
      "|    explained_variance | -22.3    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 6199     |\n",
      "|    policy_loss        | -23.9    |\n",
      "|    value_loss         | 42.9     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 67.7     |\n",
      "|    ep_rew_mean        | 46       |\n",
      "| time/                 |          |\n",
      "|    fps                | 281      |\n",
      "|    iterations         | 6300     |\n",
      "|    time_elapsed       | 111      |\n",
      "|    total_timesteps    | 31500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -4.54    |\n",
      "|    explained_variance | -5.57    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 6299     |\n",
      "|    policy_loss        | 23.4     |\n",
      "|    value_loss         | 41.9     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 67.6     |\n",
      "|    ep_rew_mean        | 45.9     |\n",
      "| time/                 |          |\n",
      "|    fps                | 282      |\n",
      "|    iterations         | 6400     |\n",
      "|    time_elapsed       | 113      |\n",
      "|    total_timesteps    | 32000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -3.92    |\n",
      "|    explained_variance | -0.225   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 6399     |\n",
      "|    policy_loss        | 5.12     |\n",
      "|    value_loss         | 3.49     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 67.4     |\n",
      "|    ep_rew_mean        | 46       |\n",
      "| time/                 |          |\n",
      "|    fps                | 282      |\n",
      "|    iterations         | 6500     |\n",
      "|    time_elapsed       | 114      |\n",
      "|    total_timesteps    | 32500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -4.07    |\n",
      "|    explained_variance | -14      |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 6499     |\n",
      "|    policy_loss        | 52.8     |\n",
      "|    value_loss         | 156      |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 67.2     |\n",
      "|    ep_rew_mean        | 45.8     |\n",
      "| time/                 |          |\n",
      "|    fps                | 283      |\n",
      "|    iterations         | 6600     |\n",
      "|    time_elapsed       | 116      |\n",
      "|    total_timesteps    | 33000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -3.99    |\n",
      "|    explained_variance | -2.13    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 6599     |\n",
      "|    policy_loss        | -3.88    |\n",
      "|    value_loss         | 2.95     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 67.2     |\n",
      "|    ep_rew_mean        | 46.6     |\n",
      "| time/                 |          |\n",
      "|    fps                | 283      |\n",
      "|    iterations         | 6700     |\n",
      "|    time_elapsed       | 118      |\n",
      "|    total_timesteps    | 33500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -4.22    |\n",
      "|    explained_variance | 0.713    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 6699     |\n",
      "|    policy_loss        | 14.5     |\n",
      "|    value_loss         | 7.46     |\n",
      "------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 66.8     |\n",
      "|    ep_rew_mean        | 46.6     |\n",
      "| time/                 |          |\n",
      "|    fps                | 284      |\n",
      "|    iterations         | 6800     |\n",
      "|    time_elapsed       | 119      |\n",
      "|    total_timesteps    | 34000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -4.04    |\n",
      "|    explained_variance | 0.959    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 6799     |\n",
      "|    policy_loss        | 18.6     |\n",
      "|    value_loss         | 33.1     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 66.4     |\n",
      "|    ep_rew_mean        | 47.1     |\n",
      "| time/                 |          |\n",
      "|    fps                | 284      |\n",
      "|    iterations         | 6900     |\n",
      "|    time_elapsed       | 121      |\n",
      "|    total_timesteps    | 34500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -4.33    |\n",
      "|    explained_variance | -2.54    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 6899     |\n",
      "|    policy_loss        | -6.15    |\n",
      "|    value_loss         | 3.98     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 65.8     |\n",
      "|    ep_rew_mean        | 47.4     |\n",
      "| time/                 |          |\n",
      "|    fps                | 285      |\n",
      "|    iterations         | 7000     |\n",
      "|    time_elapsed       | 122      |\n",
      "|    total_timesteps    | 35000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -4.04    |\n",
      "|    explained_variance | 0.218    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 6999     |\n",
      "|    policy_loss        | 10.1     |\n",
      "|    value_loss         | 9.31     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 65.3     |\n",
      "|    ep_rew_mean        | 47.7     |\n",
      "| time/                 |          |\n",
      "|    fps                | 286      |\n",
      "|    iterations         | 7100     |\n",
      "|    time_elapsed       | 124      |\n",
      "|    total_timesteps    | 35500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -4.13    |\n",
      "|    explained_variance | -2.43    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 7099     |\n",
      "|    policy_loss        | 7.89     |\n",
      "|    value_loss         | 10.4     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 64.5     |\n",
      "|    ep_rew_mean        | 50.1     |\n",
      "| time/                 |          |\n",
      "|    fps                | 286      |\n",
      "|    iterations         | 7200     |\n",
      "|    time_elapsed       | 125      |\n",
      "|    total_timesteps    | 36000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -4.04    |\n",
      "|    explained_variance | 0.682    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 7199     |\n",
      "|    policy_loss        | 7.25     |\n",
      "|    value_loss         | 3.42     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 64.5     |\n",
      "|    ep_rew_mean        | 50.7     |\n",
      "| time/                 |          |\n",
      "|    fps                | 286      |\n",
      "|    iterations         | 7300     |\n",
      "|    time_elapsed       | 127      |\n",
      "|    total_timesteps    | 36500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -3.58    |\n",
      "|    explained_variance | 0.988    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 7299     |\n",
      "|    policy_loss        | 0.952    |\n",
      "|    value_loss         | 4.39     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 64       |\n",
      "|    ep_rew_mean        | 51.4     |\n",
      "| time/                 |          |\n",
      "|    fps                | 287      |\n",
      "|    iterations         | 7400     |\n",
      "|    time_elapsed       | 128      |\n",
      "|    total_timesteps    | 37000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -4.35    |\n",
      "|    explained_variance | -0.448   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 7399     |\n",
      "|    policy_loss        | 15.9     |\n",
      "|    value_loss         | 16.7     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 63.7     |\n",
      "|    ep_rew_mean        | 51.3     |\n",
      "| time/                 |          |\n",
      "|    fps                | 287      |\n",
      "|    iterations         | 7500     |\n",
      "|    time_elapsed       | 130      |\n",
      "|    total_timesteps    | 37500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -4.13    |\n",
      "|    explained_variance | -2.09    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 7499     |\n",
      "|    policy_loss        | -12.8    |\n",
      "|    value_loss         | 12.8     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 62.5     |\n",
      "|    ep_rew_mean        | 52       |\n",
      "| time/                 |          |\n",
      "|    fps                | 288      |\n",
      "|    iterations         | 7600     |\n",
      "|    time_elapsed       | 131      |\n",
      "|    total_timesteps    | 38000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -3.55    |\n",
      "|    explained_variance | -11.9    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 7599     |\n",
      "|    policy_loss        | 7.33     |\n",
      "|    value_loss         | 8.02     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 62       |\n",
      "|    ep_rew_mean        | 52       |\n",
      "| time/                 |          |\n",
      "|    fps                | 288      |\n",
      "|    iterations         | 7700     |\n",
      "|    time_elapsed       | 133      |\n",
      "|    total_timesteps    | 38500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -3.54    |\n",
      "|    explained_variance | 0.986    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 7699     |\n",
      "|    policy_loss        | 1.42     |\n",
      "|    value_loss         | 5.68     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 62.1     |\n",
      "|    ep_rew_mean        | 52.3     |\n",
      "| time/                 |          |\n",
      "|    fps                | 288      |\n",
      "|    iterations         | 7800     |\n",
      "|    time_elapsed       | 135      |\n",
      "|    total_timesteps    | 39000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -4.14    |\n",
      "|    explained_variance | -29.2    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 7799     |\n",
      "|    policy_loss        | -13.7    |\n",
      "|    value_loss         | 81       |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 61.1     |\n",
      "|    ep_rew_mean        | 51.2     |\n",
      "| time/                 |          |\n",
      "|    fps                | 288      |\n",
      "|    iterations         | 7900     |\n",
      "|    time_elapsed       | 136      |\n",
      "|    total_timesteps    | 39500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -3.8     |\n",
      "|    explained_variance | -6.03    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 7899     |\n",
      "|    policy_loss        | -18      |\n",
      "|    value_loss         | 47.5     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 60.9     |\n",
      "|    ep_rew_mean        | 50.6     |\n",
      "| time/                 |          |\n",
      "|    fps                | 289      |\n",
      "|    iterations         | 8000     |\n",
      "|    time_elapsed       | 138      |\n",
      "|    total_timesteps    | 40000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -3.05    |\n",
      "|    explained_variance | -21.9    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 7999     |\n",
      "|    policy_loss        | 4.36     |\n",
      "|    value_loss         | 20.2     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 60.4     |\n",
      "|    ep_rew_mean        | 50.3     |\n",
      "| time/                 |          |\n",
      "|    fps                | 289      |\n",
      "|    iterations         | 8100     |\n",
      "|    time_elapsed       | 139      |\n",
      "|    total_timesteps    | 40500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.78    |\n",
      "|    explained_variance | -0.427   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 8099     |\n",
      "|    policy_loss        | 18.1     |\n",
      "|    value_loss         | 36.5     |\n",
      "------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 60.4     |\n",
      "|    ep_rew_mean        | 49.3     |\n",
      "| time/                 |          |\n",
      "|    fps                | 290      |\n",
      "|    iterations         | 8200     |\n",
      "|    time_elapsed       | 141      |\n",
      "|    total_timesteps    | 41000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -3.54    |\n",
      "|    explained_variance | -0.539   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 8199     |\n",
      "|    policy_loss        | 4.66     |\n",
      "|    value_loss         | 4.37     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 61       |\n",
      "|    ep_rew_mean        | 48.3     |\n",
      "| time/                 |          |\n",
      "|    fps                | 290      |\n",
      "|    iterations         | 8300     |\n",
      "|    time_elapsed       | 142      |\n",
      "|    total_timesteps    | 41500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -3.08    |\n",
      "|    explained_variance | -5.88    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 8299     |\n",
      "|    policy_loss        | -0.95    |\n",
      "|    value_loss         | 1.97     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 61.2     |\n",
      "|    ep_rew_mean        | 47.5     |\n",
      "| time/                 |          |\n",
      "|    fps                | 290      |\n",
      "|    iterations         | 8400     |\n",
      "|    time_elapsed       | 144      |\n",
      "|    total_timesteps    | 42000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -3.03    |\n",
      "|    explained_variance | -1.14    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 8399     |\n",
      "|    policy_loss        | 4.54     |\n",
      "|    value_loss         | 11       |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 61       |\n",
      "|    ep_rew_mean        | 46.4     |\n",
      "| time/                 |          |\n",
      "|    fps                | 291      |\n",
      "|    iterations         | 8500     |\n",
      "|    time_elapsed       | 145      |\n",
      "|    total_timesteps    | 42500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -3.77    |\n",
      "|    explained_variance | -3.84    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 8499     |\n",
      "|    policy_loss        | 10.5     |\n",
      "|    value_loss         | 9.91     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 60.8     |\n",
      "|    ep_rew_mean        | 47.1     |\n",
      "| time/                 |          |\n",
      "|    fps                | 291      |\n",
      "|    iterations         | 8600     |\n",
      "|    time_elapsed       | 147      |\n",
      "|    total_timesteps    | 43000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -3.44    |\n",
      "|    explained_variance | -5.13    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 8599     |\n",
      "|    policy_loss        | 34       |\n",
      "|    value_loss         | 72.2     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 60.9     |\n",
      "|    ep_rew_mean        | 47       |\n",
      "| time/                 |          |\n",
      "|    fps                | 292      |\n",
      "|    iterations         | 8700     |\n",
      "|    time_elapsed       | 148      |\n",
      "|    total_timesteps    | 43500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -3.82    |\n",
      "|    explained_variance | 0.677    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 8699     |\n",
      "|    policy_loss        | 14.1     |\n",
      "|    value_loss         | 11.2     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 60.6     |\n",
      "|    ep_rew_mean        | 47.3     |\n",
      "| time/                 |          |\n",
      "|    fps                | 292      |\n",
      "|    iterations         | 8800     |\n",
      "|    time_elapsed       | 150      |\n",
      "|    total_timesteps    | 44000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -3.66    |\n",
      "|    explained_variance | -17.7    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 8799     |\n",
      "|    policy_loss        | -39.9    |\n",
      "|    value_loss         | 199      |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 60.4     |\n",
      "|    ep_rew_mean        | 48.6     |\n",
      "| time/                 |          |\n",
      "|    fps                | 292      |\n",
      "|    iterations         | 8900     |\n",
      "|    time_elapsed       | 151      |\n",
      "|    total_timesteps    | 44500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -4.03    |\n",
      "|    explained_variance | -4.71    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 8899     |\n",
      "|    policy_loss        | 1.7      |\n",
      "|    value_loss         | 16.4     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 60       |\n",
      "|    ep_rew_mean        | 49.6     |\n",
      "| time/                 |          |\n",
      "|    fps                | 293      |\n",
      "|    iterations         | 9000     |\n",
      "|    time_elapsed       | 153      |\n",
      "|    total_timesteps    | 45000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -3.12    |\n",
      "|    explained_variance | -3.47    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 8999     |\n",
      "|    policy_loss        | -6.46    |\n",
      "|    value_loss         | 6.28     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 60.4     |\n",
      "|    ep_rew_mean        | 51.3     |\n",
      "| time/                 |          |\n",
      "|    fps                | 293      |\n",
      "|    iterations         | 9100     |\n",
      "|    time_elapsed       | 154      |\n",
      "|    total_timesteps    | 45500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -3.68    |\n",
      "|    explained_variance | -4.34    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 9099     |\n",
      "|    policy_loss        | -21.4    |\n",
      "|    value_loss         | 29       |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 60.2     |\n",
      "|    ep_rew_mean        | 52.8     |\n",
      "| time/                 |          |\n",
      "|    fps                | 293      |\n",
      "|    iterations         | 9200     |\n",
      "|    time_elapsed       | 156      |\n",
      "|    total_timesteps    | 46000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -4.08    |\n",
      "|    explained_variance | -12      |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 9199     |\n",
      "|    policy_loss        | 40.4     |\n",
      "|    value_loss         | 146      |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 60.1     |\n",
      "|    ep_rew_mean        | 53.3     |\n",
      "| time/                 |          |\n",
      "|    fps                | 294      |\n",
      "|    iterations         | 9300     |\n",
      "|    time_elapsed       | 158      |\n",
      "|    total_timesteps    | 46500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -3.18    |\n",
      "|    explained_variance | 0.878    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 9299     |\n",
      "|    policy_loss        | -2.69    |\n",
      "|    value_loss         | 29       |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 59.6     |\n",
      "|    ep_rew_mean        | 54.9     |\n",
      "| time/                 |          |\n",
      "|    fps                | 294      |\n",
      "|    iterations         | 9400     |\n",
      "|    time_elapsed       | 159      |\n",
      "|    total_timesteps    | 47000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -4.26    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 9399     |\n",
      "|    policy_loss        | -27.2    |\n",
      "|    value_loss         | 33.6     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 58.9     |\n",
      "|    ep_rew_mean        | 55.5     |\n",
      "| time/                 |          |\n",
      "|    fps                | 294      |\n",
      "|    iterations         | 9500     |\n",
      "|    time_elapsed       | 161      |\n",
      "|    total_timesteps    | 47500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -3.11    |\n",
      "|    explained_variance | -17.8    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 9499     |\n",
      "|    policy_loss        | 3.37     |\n",
      "|    value_loss         | 36.7     |\n",
      "------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 58.3     |\n",
      "|    ep_rew_mean        | 55.7     |\n",
      "| time/                 |          |\n",
      "|    fps                | 295      |\n",
      "|    iterations         | 9600     |\n",
      "|    time_elapsed       | 162      |\n",
      "|    total_timesteps    | 48000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -4.04    |\n",
      "|    explained_variance | -16.7    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 9599     |\n",
      "|    policy_loss        | 1.91     |\n",
      "|    value_loss         | 9.85     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 58.1     |\n",
      "|    ep_rew_mean        | 56.6     |\n",
      "| time/                 |          |\n",
      "|    fps                | 295      |\n",
      "|    iterations         | 9700     |\n",
      "|    time_elapsed       | 164      |\n",
      "|    total_timesteps    | 48500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -4.11    |\n",
      "|    explained_variance | -1.46    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 9699     |\n",
      "|    policy_loss        | 19.5     |\n",
      "|    value_loss         | 33.2     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 58       |\n",
      "|    ep_rew_mean        | 57.5     |\n",
      "| time/                 |          |\n",
      "|    fps                | 295      |\n",
      "|    iterations         | 9800     |\n",
      "|    time_elapsed       | 165      |\n",
      "|    total_timesteps    | 49000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -3.96    |\n",
      "|    explained_variance | 0.961    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 9799     |\n",
      "|    policy_loss        | -8.51    |\n",
      "|    value_loss         | 8.59     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 57.6     |\n",
      "|    ep_rew_mean        | 59.3     |\n",
      "| time/                 |          |\n",
      "|    fps                | 295      |\n",
      "|    iterations         | 9900     |\n",
      "|    time_elapsed       | 167      |\n",
      "|    total_timesteps    | 49500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -4.12    |\n",
      "|    explained_variance | 0.538    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 9899     |\n",
      "|    policy_loss        | -14      |\n",
      "|    value_loss         | 17.1     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 57.6     |\n",
      "|    ep_rew_mean        | 59.3     |\n",
      "| time/                 |          |\n",
      "|    fps                | 295      |\n",
      "|    iterations         | 10000    |\n",
      "|    time_elapsed       | 169      |\n",
      "|    total_timesteps    | 50000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -3.3     |\n",
      "|    explained_variance | -0.369   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 9999     |\n",
      "|    policy_loss        | -13.5    |\n",
      "|    value_loss         | 33.3     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 57.4     |\n",
      "|    ep_rew_mean        | 59.4     |\n",
      "| time/                 |          |\n",
      "|    fps                | 295      |\n",
      "|    iterations         | 10100    |\n",
      "|    time_elapsed       | 170      |\n",
      "|    total_timesteps    | 50500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -3.5     |\n",
      "|    explained_variance | -4.61    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 10099    |\n",
      "|    policy_loss        | -5.55    |\n",
      "|    value_loss         | 24.8     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 57.4     |\n",
      "|    ep_rew_mean        | 58.8     |\n",
      "| time/                 |          |\n",
      "|    fps                | 295      |\n",
      "|    iterations         | 10200    |\n",
      "|    time_elapsed       | 172      |\n",
      "|    total_timesteps    | 51000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -3.26    |\n",
      "|    explained_variance | 0.821    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 10199    |\n",
      "|    policy_loss        | -6.31    |\n",
      "|    value_loss         | 3.91     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 57.4     |\n",
      "|    ep_rew_mean        | 58.4     |\n",
      "| time/                 |          |\n",
      "|    fps                | 294      |\n",
      "|    iterations         | 10300    |\n",
      "|    time_elapsed       | 174      |\n",
      "|    total_timesteps    | 51500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -4.34    |\n",
      "|    explained_variance | -1.81    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 10299    |\n",
      "|    policy_loss        | 3.85     |\n",
      "|    value_loss         | 3.01     |\n",
      "------------------------------------\n"
     ]
    }
   ],
   "source": [
    "model = A2C('MultiInputPolicy', env, verbose=1).learn(200000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "85c8b094",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final task finish time : 1530\n"
     ]
    }
   ],
   "source": [
    "step = 0\n",
    "obs, _ = env.reset()\n",
    "\n",
    "while True:\n",
    "    step += 1\n",
    "    action = env.action_space.sample()\n",
    "    obs, reward, terminated, truncated, info = env.step(action)\n",
    "    done = terminated or truncated\n",
    "    if done:\n",
    "        print(f\"Final task finish time : {info['finish_time']}\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "450cabde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1708.75\n"
     ]
    }
   ],
   "source": [
    "sum_finish = 0\n",
    "\n",
    "for _ in range(1000):\n",
    "    obs, info = env.reset()\n",
    "    done = False\n",
    "\n",
    "    while True:\n",
    "        step += 1\n",
    "        action = env.action_space.sample()\n",
    "        obs, reward, terminated, truncated, info = env.step(action)\n",
    "        done = terminated or truncated\n",
    "        if done:\n",
    "            sum_finish += info['finish_time']\n",
    "            break\n",
    "            \n",
    "print(sum_finish / 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef31a0f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs, info = env.reset()\n",
    "done = False\n",
    "\n",
    "while True:\n",
    "    action, _ = model.predict(obs, deterministic = False)\n",
    "    obs, reward, terminated, truncated, info = env.step(action)\n",
    "    done = terminated or truncated\n",
    "    if done:\n",
    "        print(f\"Final task finish time : {info['finish_time']}\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "082f3351",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_finish = 0\n",
    "\n",
    "for _ in range(1000):\n",
    "    obs, info = env.reset()\n",
    "    done = False\n",
    "\n",
    "    while True:\n",
    "        action, _ = model.predict(obs, deterministic = False)\n",
    "        obs, reward, terminated, truncated, info = env.step(action)\n",
    "        done = terminated or truncated\n",
    "        if done:\n",
    "            sum_finish += info['finish_time']\n",
    "            break\n",
    "            \n",
    "print(sum_finish / 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e896ab51",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sb3_contrib import MaskablePPO\n",
    "from sb3_contrib.common.envs import InvalidActionEnvDiscrete\n",
    "from sb3_contrib.common.maskable.evaluation import evaluate_policy\n",
    "from sb3_contrib.common.maskable.utils import get_action_masks\n",
    "from sb3_contrib.common.wrappers import ActionMasker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d45ff201",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_fn(env: gym.Env):\n",
    "    # Do whatever you'd like in this function to return the action mask\n",
    "    # for the current env. In this example, we assume the env has a\n",
    "    # helpful method we can rely on.\n",
    "    return env.get_action_mask()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e46eef38",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = ActionMasker(env, mask_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cf774239",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "shape '[1, 6]' is invalid for input of size 12",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mMaskablePPO\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mMultiInputPolicy\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m50000\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sb3_contrib\\ppo_mask\\ppo_mask.py:526\u001b[0m, in \u001b[0;36mMaskablePPO.learn\u001b[1;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, use_masking, progress_bar)\u001b[0m\n\u001b[0;32m    523\u001b[0m callback\u001b[38;5;241m.\u001b[39mon_training_start(\u001b[38;5;28mlocals\u001b[39m(), \u001b[38;5;28mglobals\u001b[39m())\n\u001b[0;32m    525\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_timesteps \u001b[38;5;241m<\u001b[39m total_timesteps:\n\u001b[1;32m--> 526\u001b[0m     continue_training \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect_rollouts\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrollout_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_steps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_masking\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    528\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m continue_training:\n\u001b[0;32m    529\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sb3_contrib\\ppo_mask\\ppo_mask.py:303\u001b[0m, in \u001b[0;36mMaskablePPO.collect_rollouts\u001b[1;34m(self, env, callback, rollout_buffer, n_rollout_steps, use_masking)\u001b[0m\n\u001b[0;32m    300\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m use_masking:\n\u001b[0;32m    301\u001b[0m         action_masks \u001b[38;5;241m=\u001b[39m get_action_masks(env)\n\u001b[1;32m--> 303\u001b[0m     actions, values, log_probs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpolicy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobs_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maction_masks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maction_masks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    305\u001b[0m actions \u001b[38;5;241m=\u001b[39m actions\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[0;32m    306\u001b[0m new_obs, rewards, dones, infos \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(actions)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sb3_contrib\\common\\maskable\\policies.py:139\u001b[0m, in \u001b[0;36mMaskableActorCriticPolicy.forward\u001b[1;34m(self, obs, deterministic, action_masks)\u001b[0m\n\u001b[0;32m    137\u001b[0m distribution \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_action_dist_from_latent(latent_pi)\n\u001b[0;32m    138\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m action_masks \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 139\u001b[0m     \u001b[43mdistribution\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_masking\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction_masks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    140\u001b[0m actions \u001b[38;5;241m=\u001b[39m distribution\u001b[38;5;241m.\u001b[39mget_actions(deterministic\u001b[38;5;241m=\u001b[39mdeterministic)\n\u001b[0;32m    141\u001b[0m log_prob \u001b[38;5;241m=\u001b[39m distribution\u001b[38;5;241m.\u001b[39mlog_prob(actions)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sb3_contrib\\common\\maskable\\distributions.py:246\u001b[0m, in \u001b[0;36mMaskableMultiCategoricalDistribution.apply_masking\u001b[1;34m(self, masks)\u001b[0m\n\u001b[0;32m    243\u001b[0m     split_masks \u001b[38;5;241m=\u001b[39m th\u001b[38;5;241m.\u001b[39msplit(masks, \u001b[38;5;28mtuple\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_dims), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    245\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m distribution, mask \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdistributions, split_masks):\n\u001b[1;32m--> 246\u001b[0m     \u001b[43mdistribution\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_masking\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmask\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sb3_contrib\\common\\maskable\\distributions.py:58\u001b[0m, in \u001b[0;36mMaskableCategorical.apply_masking\u001b[1;34m(self, masks)\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m masks \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     57\u001b[0m     device \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogits\u001b[38;5;241m.\u001b[39mdevice\n\u001b[1;32m---> 58\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmasks \u001b[38;5;241m=\u001b[39m \u001b[43mth\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mas_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmasks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mth\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbool\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlogits\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     59\u001b[0m     HUGE_NEG \u001b[38;5;241m=\u001b[39m th\u001b[38;5;241m.\u001b[39mtensor(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1e8\u001b[39m, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogits\u001b[38;5;241m.\u001b[39mdtype, device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[0;32m     61\u001b[0m     logits \u001b[38;5;241m=\u001b[39m th\u001b[38;5;241m.\u001b[39mwhere(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmasks, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_original_logits, HUGE_NEG)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: shape '[1, 6]' is invalid for input of size 12"
     ]
    }
   ],
   "source": [
    "#model = MaskablePPO('MultiInputPolicy', env, verbose=1).learn(50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca318ca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs, info = env.reset()\n",
    "done = False\n",
    "episode_reward = 0\n",
    "\n",
    "while True:\n",
    "    action_masks = get_action_masks(env)\n",
    "    action, _states = model.predict(obs, action_masks=action_masks)\n",
    "\n",
    "    obs, reward, terminated, truncated, info = env.step(action)\n",
    "    done = terminated or truncated\n",
    "    episode_reward += reward\n",
    "    if done:\n",
    "        print(f\"Episode reward: {episode_reward}\")\n",
    "        print(f\"Final task finish time : {info['finish_time']}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
