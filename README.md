# RL-Scheduler: Reinforcement Learning for Repeatable Job Shop Scheduling

## ğŸ“Œ Introduction
![Scheduling Process Example](images/scheduling_process_12x8.gif)
*An example of the proposed agent's scheduling process.*


**RL-Scheduler** is a research-driven project designed to tackle the **Repeatable Job-Shop Scheduling Problem (RJSP)** using reinforcement learning. In real-world factories, the same job types are repeatedly scheduled under dynamic conditions. To handle this, we integrate a Maskable PPO-based agent with **Estimated Tardiness (ETD)**-driven priority dispatching to optimize job scheduling performance.

A full **Streamlit-based GUI** is included for setting up scheduling environments, configuring agents, training policies, and visualizing results â€” enabling end-to-end experimentation with no need to modify code manually.

> This project was developed as a 2025 graduation capstone at Pusan National University.

---



## ğŸ“‚ Project Structure

~~~bash
RL-Scheduler/
â”œâ”€â”€ pyproject.toml
â”œâ”€â”€ README.md
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ rjsp_gui/                   # Streamlit GUI app
â”‚   â”‚   â”œâ”€â”€ app.py                  # Main GUI entrypoint
â”‚   â”‚   â”œâ”€â”€ pages/                  # Streamlit page components
â”‚   â”‚   â””â”€â”€ services/               # Backend interaction logic
â”‚   â”œâ”€â”€ rl_scheduler/              # Core RL scheduling logic
â”‚   â”‚   â”œâ”€â”€ contract_generator/
â”‚   â”‚   â”œâ”€â”€ envs/                  # RJSP Gym-compatible environments
â”‚   â”‚   â”œâ”€â”€ gnn/                   # Graph Neural Network (experimental)
â”‚   â”‚   â”œâ”€â”€ graph/                 # Graph construction tools
â”‚   â”‚   â”œâ”€â”€ instances/             # Job/Machine JSON config samples
â”‚   â”‚   â”œâ”€â”€ models/                # Agent save/load utilities
â”‚   â”‚   â”œâ”€â”€ priority_rule/         # EDD, ETD, EPV-based rules
â”‚   â”‚   â”œâ”€â”€ renderer/              # Gantt chart rendering (matplotlib/plotly)
â”‚   â”‚   â”œâ”€â”€ scheduler/             # Core job/machine classes
â”‚   â”‚   â”œâ”€â”€ tests/                 # Pytest-based validation
â”‚   â”‚   â””â”€â”€ trainer/               # Training pipeline via stable-baselines3
â”‚   â””â”€â”€ rl_scheduler.egg-info/
â””â”€â”€ uv.lock
~~~

## âš™ï¸ Installation

This project uses [uv](https://github.com/astral-sh/uv) for fast, reproducible Python builds.

### Prerequisites

- Python â‰¥ 3.11
- `uv` installed:

~~~bash
curl -LsSf https://astral.sh/uv/install.sh | sh
~~~

### Setup

Install dependencies:

~~~bash
uv sync
~~~

Launch the GUI:

~~~bash
streamlit run src/rjsp_gui/app.py
~~~

## ğŸš€ How to Use (via GUI)

The GUI consists of 3 main pages:

| Page             | Tabs (Subsections)                            | Description |
|------------------|-----------------------------------------------|-------------|
| **Scheduler Setup** | `Job`, `Machine`, `Contract`                  | Upload or edit `.json` configuration files for operations, jobs, machines, and contract details. Then export as `scheduler.pkl`. |
| **Model Train**     | `Hyperparameter`, `Handler setup`, `Training`, `Visualization` | Configure PPO parameters, ETD/EDD dispatch rules, reward weighting, and start training. Track live metrics and export models (`env.pkl`, `best_model.zip`). |
| **Playground**      | â€”                                             | Load a trained agent and scheduler to run simulations interactively or evaluate actions manually/randomly. Export action sequences as `.json`. |

---

## ğŸ§  Core Techniques

- **ETD (Estimated Tardiness) Priority Rule**  
  A dynamic job urgency metric based on partial progress and estimated delays.

- **Maskable PPO-based agent**  
  Action masking ensures agents never select invalid machine-job pairs, improving stability and convergence.

- **Streamlit GUI Pipeline**  
  End-to-end management from environment building to model export â€” no code edits required.

- **Multi-scale Training & Visualization**  
  Supports dynamic sampling, Gantt chart rendering, reward tracking, and progress logs.

---

## ğŸ§ª Outputs

After training, each experiment logs the following to `/logs/{AgentName}/{timestamp}/`:

~~~bash
â”œâ”€â”€ best_model.zip
â”œâ”€â”€ env.pkl
â”œâ”€â”€ scheduler.pkl
â”œâ”€â”€ env_config.json
â”œâ”€â”€ train_config.json
â”œâ”€â”€ progress.csv
â”œâ”€â”€ model_checkpoint_*.zip
â””â”€â”€ events.out.tfevents.*   # For TensorBoard
~~~

These assets can be reloaded in the GUI's **Playground** tab or used for comparative evaluation.

---

## ğŸ“– Citation

~~~bibtex
@article{heo2024estimated,
  title={Estimated Tardiness-Based Reinforcement Learning Solution to Repeatable Job-Shop Scheduling Problems},
  author={Heo, Chi Yeong and Seo, Jun and Kim, Yonggang and Kim, Yohan and Kim, Taewoon},
  journal={Processes},
  volume={13},
  number={1},
  pages={62},
  year={2024},
  publisher={MDPI}
}
~~~
