# 0. Abstract

This winter, we embarked on a study of task scheduling problem using Reinforcement Learning (RL). Our journey began with an exploration of previous projects and research papers in the field.

We encountered two notable projects:

1. One project utilized a Monte Carlo Tree Search (MCTS) model to tackle the scheduling optimization problem. [Link](https://github.com/schrappe/mctsscheduler)
2. Another project developed a Job Shop Scheduling Problem environment tailored for Reinforcement Learning. [Link](https://github.com/prosysscience/JSSEnv)

Drawing inspiration from these endeavors, we crafted a custom environment within the stable baseline3 framework. This enabled us to apply various RL algorithms provided by sb3 to our scheduling environment.

Our study unfolded in the following stages:

1. Constructing the scheduling environment.
2. Training models using diverse RL algorithms.
3. Comparing the performance of these algorithms to identify the most effective one.
4. Conducting experiments involving adjustments such as different algorithms and hyperparameters.

The aim of our project was twofold: to probe the feasibility of employing RL in solving scheduling optimization problems and to acquire hands-on experience in this domain.
