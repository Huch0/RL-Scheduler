# 3. RL baseline3 zoo

[RL Baselines3 Zoo](https://github.com/DLR-RM/rl-baselines3-zoo) is a comprehensive training framework for Reinforcement Learning (RL) built on top of [Stable Baselines3](https://github.com/DLR-RM/stable-baselines3). It offers a range of functionalities including training, evaluating agents, hyperparameter tuning, result plotting, and video recording.

We opted to utilize this framework due to its foundation on Stable Baselines3.

## Installation

While RL Baselines3 Zoo can be installed as a Python package, we've chosen to incorporate it directly into our local project directory. This decision was made to accommodate necessary modifications to the source code and facilitate sharing among team members.

To ensure Python recognizes the package directory, you'll need to update the `PYTHONPATH` environment variable. Run the following script:

```shell
export PYTHONPATH=$PYTHONPATH:/path/to/your/package/directory
```

## Training

### Registering Custom environments

To use our custom environments, we must register them in `import_envs.py`:

```python
# For custom environments

register(id="scheduler-mctsEnv", entry_point="scheduler_env.mctsEnv:mctsEnv")
register(id="scheduler-customEnv", entry_point="scheduler_env.customEnv:SchedulingEnv")
```

We've assigned each environment an `id`, enabling access through this identifier in RL Baselines3 Zoo scripts.

### Defining hyperparameters

Hyperparameters for each environment are specified in `hyperparams/algo_name.yml`.
We appended the definitions for our custom environment to these files:

```yml
...
# for custom envs
scheduler-customEnv:
 policy: 'MultiInputPolicy'
 n_timesteps: 1000000
 ...
```

### Training a model

To train a model, execute the following command:

```shell
python train.py --algo a2c --env scheduler-customEnv --eval-freq 10000 --eval-episodes 10 --n-eval-envs 1 -n 100000
```

This trains the agent for 100,000 steps and evaluates it every 10,000 steps using 10 episodes (utilizing only one evaluation environment).
The training progress is logged, and visualizations are available.

![train](./imgs/Pasted%20image%2020240202192236.png)

`logs/a2c/scheduler-customEnv_11` is saved.
We can visualize the training curves using plotting tools.

### Plotting Training Progress

To plot training logs, run:

```shell
python plots/plot_train.py --algo a2c --env scheduler-customEnv -f logs/
```

This script plots all training logs in `logs/a2c/scheduler-customEnv_n`.

### Benchmark

To generate benchmark plots, execute:

```shell
python plots/all_plots.py --algo a2c ppo --env scheduler-customEnv -f logs -o logs/offpolicy
```

This generates a `.pkl` file containing statistics from each algorithm's logs.
Subsequently, run:

```shell
python plots/plot_from_file.py -i logs/offpolicy.pkl --rliable --skip-timesteps --versus -l a2c ppo
```

This command plots benchmark graphs created by `plots/all_plots.py`, utilizing [rliable](https://github.com/google-research/rliable).
> Ensure the `last_evals` in `.pkl` contain multiple values;
> otherwise, `plots/plot_from_file.py` won't compute Algo Scores due to insufficient evaluation data.
